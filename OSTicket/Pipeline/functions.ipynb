{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83fb10f9-947e-41d6-a176-8e36b18dc882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import *\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pytz import timezone\n",
    "import builtins\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed0ec342-a665-471c-a096-27c6365f3a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_table(df, layer, nm_table):\n",
    "    \"\"\"\n",
    "    Salva um DataFrame como tabela Delta no catálogo Spark.\n",
    "\n",
    "    Parâmetros:\n",
    "    df          : DataFrame do Spark\n",
    "    camada      : str, nome da camada ('bronze', 'silver', 'gold')\n",
    "    nome_tabela : str, nome da tabela\n",
    "\n",
    "    Retorna:\n",
    "    None (apenas imprime o status)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"osticket.{layer}.{nm_table}\")\n",
    "        print(f\"Data successfully saved to osticket.{layer}.{nm_table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc63f6ab-f710-4dc9-8bee-aa3beffef7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_table(layer, file):\n",
    "    df = spark.read.table(f\"osticket.{layer}.{file}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420387c1-2136-4bef-8efd-d011b298dee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_null(col_name):\n",
    "    return when(\n",
    "        (col(col_name).isNull()) |\n",
    "        (trim(col(col_name)) == \"\") |\n",
    "        (trim(col(col_name)) == \"None\") |\n",
    "        (trim(col(col_name)) == \"null\") |\n",
    "        (trim(col(col_name)) == \"NULL\"),\n",
    "        None\n",
    "    ).otherwise(col(col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38701f3-bf5f-494c-9558-1af228a73963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_columns(df, map):\n",
    "    for c, tipo in map.items():\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            normalize_null(c).cast(tipo)\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f783592-21fc-4004-8717-f4b4412a0909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def select_columns(p_dataframe_new, p_mapping_columns : dict):\n",
    "    \"\"\"Função para selecionar colunas. Utilizada na migração de dados da camada bronze\n",
    "    para a silver.\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    p_dataframe_new : spark.dataframe\n",
    "        Dataframe com os dados da camada bronze\n",
    "    p_mapping_columns : dict\n",
    "        Dicionário onde as chaves são os nomes das colunas a serem selecionadas\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "    spark.dataframe\n",
    "        Dataframe apenas com as colunas selecionadas\"\"\"\n",
    "    \n",
    "    return p_dataframe_new.select([col(c) for c in p_mapping_columns.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1231baf4-5669-43bf-b5da-fe1f5f940716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns(p_dataframe_new, p_mapping_columns : dict):\n",
    "    \"\"\"Função para renomear colunas. Utilizada na migração de dados da camada bronze\n",
    "    para a silver.\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    p_dataframe_new : spark.dataframe\n",
    "        Dataframe com as colunas selecionadas da camada bronze\n",
    "    p_mapping_columns : dict\n",
    "        Dicionário no formato {'old_column_name':'new_column_name'}\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "    spark.dataframe\n",
    "        Dataframe apenas com as colunas renomeadas\"\"\"\n",
    "    \n",
    "    return p_dataframe_new.select([col(f\"`{c}`\").alias(p_mapping_columns.get(c,c)) for c in p_dataframe_new.columns])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 322037385253313,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
